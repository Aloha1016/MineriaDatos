import asyncio
from twscrape import API
import pandas as pd
from pathlib import Path
from datetime import datetime

async def scrape_and_save_tweets():
    # 1. Configuraci√≥n inicial
    api = API()
    
    # 2. Agregar cuenta 
    await api.pool.add_account(
        username="TestT27133",
        password="AUDI_VID3",
        email="testeoprofundo@gmail.com",
        email_password="TestTe00",
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )

    # 3. Interfaz de usuario para par√°metros de b√∫squeda
    print("\n" + "="*50)
    print("Configuraci√≥n de B√∫squeda de Tweets")
    print("="*50)
    
    tema = input("Ingrese el hashtag/palabra clave (ej: #Python, IA, 'machine learning'): ").strip()
    a√±o = input("A√±o de b√∫squeda (ej: 2023, 2024): ").strip()
    idioma = input("Idioma (c√≥digo de 2 letras ej: es, en, fr): ").strip() or "es"
    limite = int(input("Cantidad m√°xima de tweets a obtener (ej: 100): ") or 100)
    
    # NEW: Preguntar por la subcarpeta y crearla si no existe
    db_folder = Path("DB")
    db_folder.mkdir(exist_ok=True)  # Asegurar que DB existe
    
    print("\nSubcarpetas disponibles en DB:")
    subcarpetas = [d.name for d in db_folder.iterdir() if d.is_dir()]
    print(f"üìÇ {', '.join(subcarpetas) if subcarpetas else 'Ninguna'}")
    
    subcarpeta = input("Nombre de subcarpeta (ej: Videojuegos, Futbol) o ENTER para guardar en DB/: ").strip()
    if subcarpeta:  # Si el usuario ingres√≥ algo
        subcarpeta_path = db_folder / subcarpeta
        subcarpeta_path.mkdir(exist_ok=True)  # Crear si no existe
        print(f"‚úîÔ∏è Archivo se guardar√° en: DB/{subcarpeta}/")
    else:
        subcarpeta_path = db_folder  # Guardar directamente en DB/
        print("‚úîÔ∏è Archivo se guardar√° en: DB/")

    # Construir query de b√∫squeda
    query = f"{tema} lang:{idioma} since:{a√±o}-01-01 until:{int(a√±o)+1}-01-01"
    
    print(f"\nüîé Buscando: {query}")

    # 4. Scrapear y guardar
    try:
        await api.pool.login_all()
        
        # NEW: Ruta final con subcarpeta personalizada
        next_num = len(list(subcarpeta_path.glob("DB_*.xlsx"))) + 1
        filename = subcarpeta_path / f"DB_{next_num}_{tema[:20]}_{a√±o}.xlsx"
        
        tweets = []
        async for tweet in api.search(query, limit=limite):
            tweets.append({
                "ID": tweet.id,
                "Fecha": tweet.date.strftime("%Y-%m-%d %H:%M:%S"),
                "Usuario": tweet.user.username,
                "Texto": tweet.rawContent.replace("\n", " "),
                "Likes": tweet.likeCount,
                "Retweets": tweet.retweetCount,
                "Respuestas": tweet.replyCount,
                "Idioma": idioma,
                "Tema": tema,
                "A√±o": a√±o
            })
            
            if len(tweets) % 10 == 0:
                print(f"üì• Obtenidos {len(tweets)} tweets...")
        
        if tweets:
            df = pd.DataFrame(tweets)
            df.to_excel(filename, index=False)
            print(f"\n‚úÖ {len(tweets)} tweets guardados en: {filename}")
            print(f"Muestra:\n{df.head(3)[['Fecha', 'Usuario', 'Texto']]}")
        else:
            print("‚ö†Ô∏è No se encontraron tweets con esos par√°metros")

    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        if "login" in str(e).lower():
            print("Posible soluci√≥n: Verifica tus credenciales de Twitter")

if __name__ == "__main__":
    asyncio.run(scrape_and_save_tweets())